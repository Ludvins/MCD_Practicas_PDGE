{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMofPfdv92MMEVENz2F3RU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ludvins/Practicas_PDGE/blob/master/CUDA/Suma_y_Stencil1d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99r-TI0k_V7-"
      },
      "source": [
        "# Información del sistema\n",
        "Comprobamos las características del sistema que nos ha proporcionado Google Colab. Utilizando `lscpu` podemos obtener diversas características dela CPU que utiliza el sistema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRmCimjj_eFD",
        "outputId": "4a768d00-5345-4364-b9b1-12e24beaf30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               79\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2200.000\n",
            "BogoMIPS:            4400.00\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            56320K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeqzzH3ajW0c"
      },
      "source": [
        "El comando `free` nos permite conocer información sobre la memoria física y swap del sistema. Utilizamos el flag `-h` para indicar que buscamos tener la salida en un formato más leible (Megabytes, Gygabytes...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr4-mdOu_fo9",
        "outputId": "d14742fa-1b07-4544-ba8d-b54439f5deb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!free -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G        538M         10G        972K        2.0G         11G\n",
            "Swap:            0B          0B          0B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF0YFAOb_hiB"
      },
      "source": [
        "Verificamos la versión de Cuda instalada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w30QGcoZ_kv1",
        "outputId": "55adfb7b-14be-45c4-e9fb-0f2a57d81d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stCxtcXekQmf"
      },
      "source": [
        "Utilizamos la interzaz de configuración de sistemas de NVIDIA (NVIDIA System Management Interface) para conocer el estado de la tarjeta gráfica que vamos a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7oDGUCF_mBK",
        "outputId": "9ae8c260-d5bd-474c-d3c8-13e11a48af5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Oct 16 15:19:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygCGApW0kmMa"
      },
      "source": [
        "Podemos ejecutar el codigo de ejemplo presente en la libreria de Cuda que nos enumera las propeidades del dispositivo Cuda que existe en el sistema. Para ello, cambiamos de directorio a aquel donde se encuentra el codigo fuente:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egav_e4ULqaQ",
        "outputId": "21d048c0-870e-4da0-deb4-58b751edef50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /usr/local/cuda/samples/1_Utilities/deviceQuery/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_heYpiyk_ow"
      },
      "source": [
        "El programa no se encuentra compilado, pero provee de un makefile para su facil compilación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6sfqdvQLtAX",
        "outputId": "110fc05b-9dee-42fe-e395-ff1453c816ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!make"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery.o -c deviceQuery.cpp\n",
            "/usr/local/cuda-10.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery deviceQuery.o \n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../bin/x86_64/linux/release\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuXDl-JflIfY"
      },
      "source": [
        "Si ejecutamos el programa, obtenemos información interesante, como que el número máximo de threads por bloque que podemos utilizar es 1024:\n",
        "```\n",
        "Maximum number of threads per block: 1024\n",
        "```\n",
        "También vemos la dimención máxima que podemos dar a cada dimensión de bloque y grid:\n",
        "```\n",
        "Max dimension size of a thread block (x,y,z): (1024, 1024, 64),\n",
        "Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKGfxLn7Lwje",
        "outputId": "411fc2c5-5707-43de-d414-5049693515f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "!!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./deviceQuery Starting...',\n",
              " '',\n",
              " ' CUDA Device Query (Runtime API) version (CUDART static linking)',\n",
              " '',\n",
              " 'Detected 1 CUDA Capable device(s)',\n",
              " '',\n",
              " 'Device 0: \"Tesla P100-PCIE-16GB\"',\n",
              " '  CUDA Driver Version / Runtime Version          10.1 / 10.1',\n",
              " '  CUDA Capability Major/Minor version number:    6.0',\n",
              " '  Total amount of global memory:                 16281 MBytes (17071734784 bytes)',\n",
              " '  (56) Multiprocessors, ( 64) CUDA Cores/MP:     3584 CUDA Cores',\n",
              " '  GPU Max Clock rate:                            1329 MHz (1.33 GHz)',\n",
              " '  Memory Clock rate:                             715 Mhz',\n",
              " '  Memory Bus Width:                              4096-bit',\n",
              " '  L2 Cache Size:                                 4194304 bytes',\n",
              " '  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)',\n",
              " '  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers',\n",
              " '  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers',\n",
              " '  Total amount of constant memory:               65536 bytes',\n",
              " '  Total amount of shared memory per block:       49152 bytes',\n",
              " '  Total number of registers available per block: 65536',\n",
              " '  Warp size:                                     32',\n",
              " '  Maximum number of threads per multiprocessor:  2048',\n",
              " '  Maximum number of threads per block:           1024',\n",
              " '  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)',\n",
              " '  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)',\n",
              " '  Maximum memory pitch:                          2147483647 bytes',\n",
              " '  Texture alignment:                             512 bytes',\n",
              " '  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)',\n",
              " '  Run time limit on kernels:                     No',\n",
              " '  Integrated GPU sharing Host Memory:            No',\n",
              " '  Support host page-locked memory mapping:       Yes',\n",
              " '  Alignment requirement for Surfaces:            Yes',\n",
              " '  Device has ECC support:                        Enabled',\n",
              " '  Device supports Unified Addressing (UVA):      Yes',\n",
              " '  Device supports Compute Preemption:            Yes',\n",
              " '  Supports Cooperative Kernel Launch:            Yes',\n",
              " '  Supports MultiDevice Co-op Kernel Launch:      Yes',\n",
              " '  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4',\n",
              " '  Compute Mode:',\n",
              " '     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >',\n",
              " '',\n",
              " 'deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1',\n",
              " 'Result = PASS']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2CtfWTcAASW"
      },
      "source": [
        "# Ejercicio 1\n",
        "\n",
        "El código de ejemplo en suma de los elementos de un vector realiza la\n",
        "suma de dos vectores en la GPU.\n",
        "1. Comente los diferentes casos propuestos en el ejemplo y\n",
        "conteste a las preguntas.\n",
        "2. Se propone extender el código de este ejemplo para que realice\n",
        "la resta (o suma) de matrices cuadradas de dimensión N. \n",
        "  - Configure adecuadamente el Grid de threads para aceptar\n",
        "matrices de cualquier tamaño.\n",
        "  - En el kernel, utilice las variables blockIdx y threadIdx\n",
        "adecuadamente para acceder a una estructura bidimensional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmyV4CtLGm1v"
      },
      "source": [
        "### Suma de Vectores completa\n",
        "\n",
        "Vemos a continuación el código concreto que permite sumar dos vectores guardando el resultado en el segundo de estos.\n",
        "\n",
        "En él, utilizamos un total de 512 threads por bloque y (N + THREADS_PER_BLOCK - 1)/THREADS_PER_BLOCK bloques distintos. De esta forma, si N es multiplo de THREADS_PER_BLOCK\n",
        "$$\n",
        "\\frac{N + TPB - 1}{TPB} = \\frac{N}{TPB} + 1 - \\frac{1}{TPB} = K + 1 -\\frac{1}{TPB}\n",
        "$$\n",
        "Lo cual tras transformarlo a entero resultaria en $K = N/TPB$. En caso de no ser multiplos, $N$ se descompone como $N_1 + N_2$ donde $N_1$ si es multiplo de $TPB$ y $0 < N_2 < TPB$:\n",
        "$$\n",
        "\\frac{N + TPB - 1}{TPB} = \\frac{N_1}{TPB} + \\frac{N_2}{TPB} + 1 - \\frac{1}{TPB} = K + 1 + \\frac{N_2}{TPB} -\\frac{1}{TPB}\n",
        "$$\n",
        "Por ser $0 < N_2 < TPB$, luego \n",
        "$$\n",
        "\\frac{N_2}{TPB} -\\frac{1}{TPB} \\geq 0\n",
        "$$\n",
        "Resultando en $K + 1$ iteraciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m90eOq9eAaDM",
        "outputId": "b0565089-44b9-4d6f-f701-c44e167866fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile suma_vec.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// Define array size\n",
        "#define N (1024*1024)\n",
        "// Define number of threads per block\n",
        "#define THREADS_PER_BLOCK 512\n",
        "\n",
        "// Función de suma de vectores en GPU.\n",
        "__global__ void add(float *x, float *y) {\n",
        "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    // Comprobamos que el índice es correcto\n",
        "    if (i < N )\n",
        "        y[i]=x[i]+y[i];\n",
        "}\n",
        "\n",
        "\n",
        "// Función principal (CPU)\n",
        "int main(void) {\n",
        "    // Arrays en CPU\n",
        "    float *x, *y;                 // = new float[N];\n",
        "    // Arrays en GPU\n",
        "    float *d_x, *d_y;             // = new float[N]; \n",
        "    // Tamaño de los arrays\n",
        "    int size = N*sizeof(float);\n",
        "\n",
        "    // Reservamos memoria en el dispositivo.\n",
        "    cudaMalloc((void **)&d_x, size);\n",
        "    cudaMalloc((void **)&d_y, size);\n",
        "\n",
        "    // Reservamos memoria en el host\n",
        "    x = (float *)malloc(size);\n",
        "    y = (float *)malloc(size);\n",
        "\n",
        "    // Inicializamos los vectores\n",
        "    for (int i =0; i < N; i++ ){\n",
        "        x[i]= 1.0f;\n",
        "        y[i]= 2.0f;\n",
        "    }\n",
        "\n",
        "    // Copiamos los valores al dispositivo\n",
        "    cudaMemcpy(d_x, x, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_y, y, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Lanzamos add\n",
        "    add<<<(N + THREADS_PER_BLOCK - 1)/THREADS_PER_BLOCK, THREADS_PER_BLOCK>>>(d_x, d_y);\n",
        "\n",
        "    // Esperamos que se realicen todas las iteraciones.\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copiamos el resultado al host\n",
        "    cudaMemcpy(y, d_y, size, cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    // Comprobamos los resultados\n",
        "    float maxError = 0.0f;\n",
        "    int contError = 0;\n",
        "   \n",
        "    for (int i=0; i <N; i++){\n",
        "        maxError=fmax(maxError,fabs(y[i]-3.0f));\n",
        "        if (y[i] != 3.0) contError++; \n",
        "    }\n",
        "    std::cout << \"suma de \" << N << \" Elementos\" << std::endl;\n",
        "    std::cout << \"Número de Errores: \" <<contError << std::endl;\n",
        "    std::cout << \"Max error: \" <<maxError << std::endl;\n",
        "\n",
        "    free(x); free(y);\n",
        "    cudaFree(d_x); cudaFree(d_y);\n",
        "   return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting suma_vec.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siRfJpVzA1U0"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma_vec.cu -o suma_vec -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRAyRqQ7A1qi",
        "outputId": "cbe43fc1-6b9f-4c5b-cad9-0769237aa5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!./suma_vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "suma de 1048576 Elementos\n",
            "Número de Errores: 0\n",
            "Max error: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CrNSHn7A6uU",
        "outputId": "850d024c-6228-45ca-dc1c-f2426c075cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!nvprof ./suma_vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==1124== NVPROF is profiling process 1124, command: ./suma_vec\n",
            "suma de 4194304 Elementos\n",
            "Número de Errores: 0\n",
            "Max error: 0\n",
            "==1124== Profiling application: ./suma_vec\n",
            "==1124== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   58.64%  9.3680ms         1  9.3680ms  9.3680ms  9.3680ms  [CUDA memcpy DtoH]\n",
            "                   40.77%  6.5130ms         2  3.2565ms  3.2527ms  3.2603ms  [CUDA memcpy HtoD]\n",
            "                    0.59%  94.558us         1  94.558us  94.558us  94.558us  add(float*, float*, float*)\n",
            "      API calls:   88.40%  154.30ms         3  51.433ms  88.906us  154.11ms  cudaMalloc\n",
            "                   10.01%  17.474ms         3  5.8247ms  3.4203ms  10.582ms  cudaMemcpy\n",
            "                    1.16%  2.0296ms         3  676.54us  167.66us  1.1382ms  cudaFree\n",
            "                    0.21%  362.95us         1  362.95us  362.95us  362.95us  cuDeviceTotalMem\n",
            "                    0.09%  165.22us        97  1.7030us     133ns  54.388us  cuDeviceGetAttribute\n",
            "                    0.09%  153.80us         1  153.80us  153.80us  153.80us  cudaDeviceSynchronize\n",
            "                    0.02%  32.859us         1  32.859us  32.859us  32.859us  cudaLaunchKernel\n",
            "                    0.01%  16.210us         1  16.210us  16.210us  16.210us  cuDeviceGetName\n",
            "                    0.00%  2.9630us         1  2.9630us  2.9630us  2.9630us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1790us         3     726ns     201ns  1.4800us  cuDeviceGetCount\n",
            "                    0.00%  1.4460us         2     723ns     316ns  1.1300us  cuDeviceGet\n",
            "                    0.00%     267ns         1     267ns     267ns     267ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWb6N439J73j"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8w-6yuRKok2"
      },
      "source": [
        "Si ejecutamos el programa utilizando un total de 256 threads por bloque y un solo bloque, es decir:\n",
        "```\n",
        "add<<<1, 256>>>(d_x, d_y);\n",
        "```\n",
        "Obtendríamos que solo se ha realizado la operación sobre 256 valores de los N posibles, esto mismo pasa utilizando 256 bloques y un thread en cada uno. \n",
        "\n",
        "Por otro lado, si ejecutamos dicha función sin quitar el bucle for que teníamos inicialmente en la función add:\n",
        "\n",
        "```\n",
        "__global__ void add(int n, float *x, float *y) {\n",
        "    for (int i =0; i < n; i++ ){\n",
        "        y[i]=x[i]+y[i];\n",
        "    }\n",
        "}\n",
        "```\n",
        "obtendríamos que cada thread lanzado realizaría todas las sumas, pudiendo ocurrir que se pisaran unas a otras\n",
        "\n",
        "```\n",
        "Suma de 1048576 elementos\n",
        "Número de errores: 3473\n",
        "Max error: 2\n",
        "```\n",
        "\n",
        "**Pendiente: Comentar <1,256> sin datarace y suma en la GPU con paralelismo de bloques.Prueba con mas Thread por bloque. Pruebas variando N y la complejidad de la\n",
        "operación matemática.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKdkqK87NQDw"
      },
      "source": [
        "## Suma de matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etw0Z2fcGlaD",
        "outputId": "b62abcf5-38cf-4f71-be46-f68757010f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile suma_mat.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "// Definimos las dimensiones de las matrices\n",
        "#define R 512\n",
        "#define C 512\n",
        "\n",
        "// __global__ indica que se ejecuta en el dispositivo, por lo tanto\n",
        "// x, y,deben apuntar a memoria en el dispositivo.\n",
        "\n",
        "__global__ void add(float *x, float *y) {\n",
        "    \n",
        "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int j = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int index = i*R + j;\n",
        "    if (index < R*C)\n",
        "      y[index] = x[index] + y[index];\n",
        "}\n",
        "\n",
        "\n",
        "// Define number of threads per block\n",
        "#define THREADS_PER_BLOCK 32\n",
        "\n",
        "int main(void) {\n",
        "    // Inicializamos las variables\n",
        "    float *x, *y;                 // = new float[N];\n",
        "    float *d_x, *d_y;             // = new float[N]; \n",
        "    // Calculamos el tamaño de las matrices\n",
        "    int size = C*R*sizeof(float);\n",
        "\n",
        "    // Reservamos memoria en el dispositivo.\n",
        "    cudaMalloc((void **)&d_x, size);\n",
        "    cudaMalloc((void **)&d_y, size);\n",
        "\n",
        "    // Reservamos memoria en el host, trataremos las matrices como vectores\n",
        "    // con la dimension correspondiente y accederemos a ellos usando 2 indices.\n",
        "    x = (float *)malloc(R*C*sizeof(float*));\n",
        "    y = (float *)malloc(R*C*sizeof(float*));\n",
        "\n",
        "    // Inicializamos los datos\n",
        "    for (int i =0; i < R; i++ ){\n",
        "        for (int j = 0; j < C; j++){\n",
        "            x[i*R + j] = 1.0f;\n",
        "            y[i*R + j] = 2.0f;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Copiamos los valores al dispositivo\n",
        "    cudaMemcpy(d_x, x, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_y, y, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Creamos los objetos dim3 tanto para el Grid como para el bloque.\n",
        "    dim3 dimGrid ( (R + THREADS_PER_BLOCK - 1)/THREADS_PER_BLOCK , (C + THREADS_PER_BLOCK - 1)/THREADS_PER_BLOCK ,1 ) ;\n",
        "    // Definimos la dimension del bloque.\n",
        "    dim3 dimBlock( THREADS_PER_BLOCK, THREADS_PER_BLOCK, 1 ) ;\n",
        "\n",
        "    // Lamzamos la función en el dispositivo.\n",
        "    add<<<dimGrid, dimBlock>>>(d_x, d_y);\n",
        "\n",
        "    // Esperamos que se realicen todas las operaciones.\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copiamos el resultado al host.\n",
        "    cudaMemcpy(y, d_y, size, cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    // Comprobamos los resultados\n",
        "    float maxError = 0.0f;\n",
        "    int contError = 0;\n",
        "   \n",
        "    for (int i=0; i <R; i++){\n",
        "        for (int j = 0; j < C; j++){\n",
        "          maxError=fmax(maxError,fabs(y[i*R + j]-3.0f));\n",
        "          if (y[i*R + j] != 3.0) contError++; \n",
        "        }\n",
        "    }\n",
        "    std::cout << \"Suma de \" << R*C << \" Elementos\" << std::endl;\n",
        "    std::cout << \"Número de Errores: \" << contError << std::endl;\n",
        "    std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "    // Liberamos la memoria tanto en el host como en el dispositivo.\n",
        "    free(x); free(y);\n",
        "    cudaFree(d_x); cudaFree(d_y);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting suma_mat.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvCVjyGaI4MU"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma_mat.cu -o suma_mat -lcudadevrt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrxmZhL2L97F",
        "outputId": "6dc211ea-2822-4f22-b5e7-51436019fed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!./suma_mat"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "suma de 262144 Elementos\n",
            "Número de Errores: 0\n",
            "Max error: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_X9ir7X3C2N"
      },
      "source": [
        "## Spencil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fui3Dwac8ras"
      },
      "source": [
        "Comenzamos con la versión del spencil sin memoria compartida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3QOalW-8vTe",
        "outputId": "e78a3843-db80-43b6-eee9-6f5a367c311a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 1destencilexercise_v1.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) {\n",
        "    // Indice global de la posicion central de los datos del spencil que va a usar el thread.\n",
        "    int index = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "\n",
        "    // Realizamos la operación del spencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS; offset <= RADIUS; offset++)\n",
        "        result += in[index + offset];\n",
        "\n",
        "    // Guardamos el resultado\n",
        "    out[index-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Incializamos los datos en el host\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // Con un valor de 1 y un radio de 3, el array de resultados deberia ser de 7's.\n",
        "\n",
        "  // Reservamos memoria en el dispositivo\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copiamos los datos en la GPU\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // Aplicamos el spencil\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  // Copiamos los resultados\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verificamos que se ha hecho correctamente\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Liberamos memoria\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 1destencilexercise_v1.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZIbKVxZ83NT",
        "outputId": "7541cc59-6ec7-459b-a4a2-a87ba6bc0b2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 1destencilexercise_v1.cu -o 1destencilexercise -lcudadevrt\n",
        "!./1destencilexercise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUCCESS!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCgjJYPn9EL6"
      },
      "source": [
        "Comprobamos el comportamiento temporal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBXiKoal9DHy",
        "outputId": "7670b839-fbe5-4498-9e3a-83cc8ba7ef4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!nvprof ./1destencilexercise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==575== NVPROF is profiling process 575, command: ./1destencilexercise\n",
            "SUCCESS!\n",
            "==575== Profiling application: ./1destencilexercise\n",
            "==575== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   37.81%  4.4160us         1  4.4160us  4.4160us  4.4160us  [CUDA memcpy DtoH]\n",
            "                   36.16%  4.2240us         1  4.2240us  4.2240us  4.2240us  [CUDA memcpy HtoD]\n",
            "                   26.03%  3.0400us         1  3.0400us  3.0400us  3.0400us  stencil_1d(int*, int*)\n",
            "      API calls:   99.61%  205.42ms         2  102.71ms  7.3560us  205.41ms  cudaMalloc\n",
            "                    0.18%  367.52us         1  367.52us  367.52us  367.52us  cuDeviceTotalMem\n",
            "                    0.07%  152.23us        97  1.5690us     145ns  64.354us  cuDeviceGetAttribute\n",
            "                    0.06%  127.92us         2  63.961us  14.654us  113.27us  cudaFree\n",
            "                    0.04%  84.581us         2  42.290us  33.218us  51.363us  cudaMemcpy\n",
            "                    0.02%  31.233us         1  31.233us  31.233us  31.233us  cuDeviceGetName\n",
            "                    0.01%  25.791us         1  25.791us  25.791us  25.791us  cudaLaunchKernel\n",
            "                    0.00%  3.5840us         1  3.5840us  3.5840us  3.5840us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8140us         3     604ns     171ns  1.1390us  cuDeviceGetCount\n",
            "                    0.00%  1.2090us         2     604ns     310ns     899ns  cuDeviceGet\n",
            "                    0.00%     290ns         1     290ns     290ns     290ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxnzXBR5_FIv"
      },
      "source": [
        "Al no utilizar memoria compartida, la mayoria del tiempo se dedica a copia de elementos.\n",
        "\n",
        "**TODO**: 2. Use el generador de perfiles para determinar cuál es el problema.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2aMdHHDApkM"
      },
      "source": [
        "## Versión con memoria compartida sin sincronización de threads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV_FLBxDRxaN",
        "outputId": "cb9cc700-8075-4e52-88be-334a505c9004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 1destencilexercise_v2.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    // Reservamos la memoria compartida\n",
        "    __shared__ int temp[BLOCK_SIZE + 2*RADIUS];\n",
        "    // Indice central en el array global\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    // Indice centran en el array local\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Leemos los puntos a la memoria compartida\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Hacemos la operación\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Guardamos el resultado\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "  // Ejecutamos el código 100 veces independientes\n",
        "  for (int j = 0; j < 100; j++){\n",
        "      unsigned int i;\n",
        "      int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "      int *d_in, *d_out;\n",
        "\n",
        "      // Inicializamos los datos\n",
        "      for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "          h_in[i] = 1; \n",
        "\n",
        "      // Reserva memoria en la GPU\n",
        "      cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "      cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "      // Copiar los datos al dispositivo\n",
        "      cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "      stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "      cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "      for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "        if (h_out[i] != 7)\n",
        "        {\n",
        "          printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "          break;\n",
        "        }\n",
        "\n",
        "      // Free out memory\n",
        "      cudaFree(d_in);\n",
        "      cudaFree(d_out);\n",
        "      cudaDeviceReset();\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 1destencilexercise_v2.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBv_sOG636b"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 1destencilexercise_v2.cu -o 1destencilexercise -lcudadevrt"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez-d6A9M6-pb",
        "outputId": "016dd6b8-569f-4902-f25e-b28a8fa184bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!./1destencilexercise"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7FO3dy9AwIq"
      },
      "source": [
        "## Versión con memoria compartida y sincronización de threads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_ILjJYw7wu9",
        "outputId": "004c3a53-3060-4258-e205-ed6e4d10fd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 1destencilexercise_v3.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2*RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "    __syncthreads ();\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "      unsigned int i;\n",
        "      int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "      int *d_in, *d_out;\n",
        "\n",
        "    // Initialize host data\n",
        "    for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "      h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "    // Allocate space on the device\n",
        "    cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "    cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "    // Copy input data to device\n",
        "    cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "    stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "    cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "    // Verify every out value is 7\n",
        "    for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "      if (h_out[i] != 7)\n",
        "      {\n",
        "        printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "        break;\n",
        "      }\n",
        " \n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "\n",
        "    // Free out memory\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing 1destencilexercise_v3.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgOtQ22UDzQ1",
        "outputId": "00e6ba8c-f833-4e00-d8dc-6163f71b5e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 1destencilexercise_v3.cu -o 1destencilexercise -lcudadevrt\n",
        "!./1destencilexercise"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUCCESS!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Re8KVmD263",
        "outputId": "12be9d22-bae6-4264-d437-64d3e9c65ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!nvprof ./1destencilexercise"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==881== NVPROF is profiling process 881, command: ./1destencilexercise\n",
            "SUCCESS!\n",
            "==881== Profiling application: ./1destencilexercise\n",
            "==881== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   37.06%  4.3520us         1  4.3520us  4.3520us  4.3520us  [CUDA memcpy DtoH]\n",
            "                   35.97%  4.2240us         1  4.2240us  4.2240us  4.2240us  [CUDA memcpy HtoD]\n",
            "                   26.98%  3.1680us         1  3.1680us  3.1680us  3.1680us  stencil_1d(int*, int*)\n",
            "      API calls:   99.61%  192.38ms         2  96.188ms  6.8640us  192.37ms  cudaMalloc\n",
            "                    0.20%  382.92us         1  382.92us  382.92us  382.92us  cuDeviceTotalMem\n",
            "                    0.07%  144.48us        97  1.4890us     164ns  59.230us  cuDeviceGetAttribute\n",
            "                    0.05%  100.27us         2  50.132us  12.964us  87.301us  cudaFree\n",
            "                    0.04%  71.811us         2  35.905us  30.655us  41.156us  cudaMemcpy\n",
            "                    0.01%  28.067us         1  28.067us  28.067us  28.067us  cuDeviceGetName\n",
            "                    0.01%  26.077us         1  26.077us  26.077us  26.077us  cudaLaunchKernel\n",
            "                    0.00%  3.7350us         1  3.7350us  3.7350us  3.7350us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7960us         3     598ns     170ns  1.1880us  cuDeviceGetCount\n",
            "                    0.00%  1.4130us         2     706ns     374ns  1.0390us  cuDeviceGet\n",
            "                    0.00%     290ns         1     290ns     290ns     290ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rArj_p1EPW1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
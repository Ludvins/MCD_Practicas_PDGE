{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOf2iAsv6V2PnA+X0f6tloW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ludvins/Practicas_PDGE/blob/master/CUDA/P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99r-TI0k_V7-"
      },
      "source": [
        "# Información del sistema\n",
        "Comprobamos las características del sistema que nos ha proporcionado Google Colab. Utilizando `lscpu` podemos obtener diversas características dela CPU que utiliza el sistema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRmCimjj_eFD",
        "outputId": "4a768d00-5345-4364-b9b1-12e24beaf30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               79\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2200.000\n",
            "BogoMIPS:            4400.00\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            56320K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeqzzH3ajW0c"
      },
      "source": [
        "El comando `free` nos permite conocer información sobre la memoria física y swap del sistema. Utilizamos el flag `-h` para indicar que buscamos tener la salida en un formato más leible (Megabytes, Gygabytes...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr4-mdOu_fo9",
        "outputId": "d14742fa-1b07-4544-ba8d-b54439f5deb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!free -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G        538M         10G        972K        2.0G         11G\n",
            "Swap:            0B          0B          0B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF0YFAOb_hiB"
      },
      "source": [
        "Verificamos la versión de Cuda instalada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w30QGcoZ_kv1",
        "outputId": "55adfb7b-14be-45c4-e9fb-0f2a57d81d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stCxtcXekQmf"
      },
      "source": [
        "Utilizamos la interzaz de configuración de sistemas de NVIDIA (NVIDIA System Management Interface) para conocer el estado de la tarjeta gráfica que vamos a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7oDGUCF_mBK",
        "outputId": "9ae8c260-d5bd-474c-d3c8-13e11a48af5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Oct 16 15:19:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygCGApW0kmMa"
      },
      "source": [
        "Podemos ejecutar el codigo de ejemplo presente en la libreria de Cuda que nos enumera las propeidades del dispositivo Cuda que existe en el sistema. Para ello, cambiamos de directorio a aquel donde se encuentra el codigo fuente:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egav_e4ULqaQ",
        "outputId": "21d048c0-870e-4da0-deb4-58b751edef50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /usr/local/cuda/samples/1_Utilities/deviceQuery/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_heYpiyk_ow"
      },
      "source": [
        "El programa no se encuentra compilado, pero provee de un makefile para su facil compilación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6sfqdvQLtAX",
        "outputId": "110fc05b-9dee-42fe-e395-ff1453c816ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!make"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery.o -c deviceQuery.cpp\n",
            "/usr/local/cuda-10.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery deviceQuery.o \n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../bin/x86_64/linux/release\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuXDl-JflIfY"
      },
      "source": [
        "Si ejecutamos el programa, obtenemos información interesante, como que el número máximo de threads por bloque que podemos utilizar es 1024:\n",
        "```\n",
        "Maximum number of threads per block: 1024\n",
        "```\n",
        "También vemos la dimención máxima que podemos dar a cada dimensión de bloque y grid:\n",
        "```\n",
        "Max dimension size of a thread block (x,y,z): (1024, 1024, 64),\n",
        "Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKGfxLn7Lwje",
        "outputId": "411fc2c5-5707-43de-d414-5049693515f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "!!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./deviceQuery Starting...',\n",
              " '',\n",
              " ' CUDA Device Query (Runtime API) version (CUDART static linking)',\n",
              " '',\n",
              " 'Detected 1 CUDA Capable device(s)',\n",
              " '',\n",
              " 'Device 0: \"Tesla P100-PCIE-16GB\"',\n",
              " '  CUDA Driver Version / Runtime Version          10.1 / 10.1',\n",
              " '  CUDA Capability Major/Minor version number:    6.0',\n",
              " '  Total amount of global memory:                 16281 MBytes (17071734784 bytes)',\n",
              " '  (56) Multiprocessors, ( 64) CUDA Cores/MP:     3584 CUDA Cores',\n",
              " '  GPU Max Clock rate:                            1329 MHz (1.33 GHz)',\n",
              " '  Memory Clock rate:                             715 Mhz',\n",
              " '  Memory Bus Width:                              4096-bit',\n",
              " '  L2 Cache Size:                                 4194304 bytes',\n",
              " '  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)',\n",
              " '  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers',\n",
              " '  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers',\n",
              " '  Total amount of constant memory:               65536 bytes',\n",
              " '  Total amount of shared memory per block:       49152 bytes',\n",
              " '  Total number of registers available per block: 65536',\n",
              " '  Warp size:                                     32',\n",
              " '  Maximum number of threads per multiprocessor:  2048',\n",
              " '  Maximum number of threads per block:           1024',\n",
              " '  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)',\n",
              " '  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)',\n",
              " '  Maximum memory pitch:                          2147483647 bytes',\n",
              " '  Texture alignment:                             512 bytes',\n",
              " '  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)',\n",
              " '  Run time limit on kernels:                     No',\n",
              " '  Integrated GPU sharing Host Memory:            No',\n",
              " '  Support host page-locked memory mapping:       Yes',\n",
              " '  Alignment requirement for Surfaces:            Yes',\n",
              " '  Device has ECC support:                        Enabled',\n",
              " '  Device supports Unified Addressing (UVA):      Yes',\n",
              " '  Device supports Compute Preemption:            Yes',\n",
              " '  Supports Cooperative Kernel Launch:            Yes',\n",
              " '  Supports MultiDevice Co-op Kernel Launch:      Yes',\n",
              " '  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4',\n",
              " '  Compute Mode:',\n",
              " '     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >',\n",
              " '',\n",
              " 'deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1',\n",
              " 'Result = PASS']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2CtfWTcAASW"
      },
      "source": [
        "# Ejercicio 1\n",
        "\n",
        "El código de ejemplo en suma de los elementos de un vector realiza la\n",
        "suma de dos vectores en la GPU.\n",
        "1. Comente losdiferentes casos propuestos en el ejemplo y\n",
        "conteste a las preguntas.\n",
        "2. Se propone extender el código de este ejemplo para que realice\n",
        "la resta (o suma) de matrices cuadradas de dimensión N. \n",
        "  - Configure adecuadamente el Grid de threads para aceptar\n",
        "matrices de cualquier tamaño.\n",
        "  - En el kernel, utilice las variables blockIdx y threadIdx\n",
        "adecuadamente para acceder a una estructura bidimensional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmyV4CtLGm1v"
      },
      "source": [
        "### Suma de Vectores completa\n",
        "\n",
        "Vemos a continuación el código concreto que permite sumar dos vectores guardando el resultado en el segundo de estos.\n",
        "\n",
        "En él, utilizamos un total de 512 threads por bloque."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m90eOq9eAaDM",
        "outputId": "b0565089-44b9-4d6f-f701-c44e167866fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile suma_vec.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// __global__ indica que se ejecuta en el dispositivo, por lo tanto\n",
        "// x, y,deben apuntar a memoria en el dispositivo.\n",
        "__global__ void add(float *x, float *y, int size) {\n",
        "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (i < size )\n",
        "        y[i]=x[i]+y[i];\n",
        "}\n",
        "\n",
        "// Define array size\n",
        "#define N (1024*1024)\n",
        "// Define number of threads per block\n",
        "#define THREADS_PER_BLOCK 512\n",
        "\n",
        "int main(void) {\n",
        "    // Numero de datos\n",
        "    float *x, *y;                 // = new float[N];\n",
        "    float *d_x, *d_y;             // = new float[N]; \n",
        "    int size = N*sizeof(float);\n",
        "\n",
        "    // Reservamos memoria en el dispositivo.\n",
        "    cudaMalloc((void **)&d_x, size);\n",
        "    cudaMalloc((void **)&d_y, size);\n",
        "\n",
        "    //Reservamos moemoria en el host\n",
        "    x = (float *)malloc(size);\n",
        "    y = (float *)malloc(size);\n",
        "    for (int i =0; i < N; i++ ){\n",
        "        x[i]= 1.0f;\n",
        "        y[i]= 2.0f;\n",
        "    }\n",
        "\n",
        "    // Copiamos los valores al dispositivo\n",
        "    cudaMemcpy(d_x, x, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_y, y, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Lanzamos add\n",
        "    add<<<N/THREADS_PER_BLOCK, THREADS_PER_BLOCK>>>(d_x, d_y, N);\n",
        "\n",
        "    // Esperamos que se realicen todas las iteraciones.\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copiamos el resultado al host\n",
        "    cudaMemcpy(y, d_y, size, cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    // Comprobamos los resultados\n",
        "    float maxError = 0.0f;\n",
        "    int contError = 0;\n",
        "   \n",
        "    for (int i=0; i <N; i++){\n",
        "        maxError=fmax(maxError,fabs(y[i]-3.0f));\n",
        "        if (y[i] != 3.0) contError++; \n",
        "    }\n",
        "    std::cout << \"suma de \" << N << \" Elementos\" << std::endl;\n",
        "    std::cout << \"Número de Errores: \" <<contError << std::endl;\n",
        "    std::cout << \"Max error: \" <<maxError << std::endl;\n",
        "\n",
        "    free(x); free(y);\n",
        "    cudaFree(d_x); cudaFree(d_y);\n",
        "   return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting suma_vec.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siRfJpVzA1U0"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma_vec.cu -o suma_vec -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRAyRqQ7A1qi",
        "outputId": "cbe43fc1-6b9f-4c5b-cad9-0769237aa5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!./suma_vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "suma de 1048576 Elementos\n",
            "Número de Errores: 0\n",
            "Max error: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CrNSHn7A6uU",
        "outputId": "850d024c-6228-45ca-dc1c-f2426c075cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!nvprof ./suma_vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==1124== NVPROF is profiling process 1124, command: ./suma_vec\n",
            "suma de 4194304 Elementos\n",
            "Número de Errores: 0\n",
            "Max error: 0\n",
            "==1124== Profiling application: ./suma_vec\n",
            "==1124== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   58.64%  9.3680ms         1  9.3680ms  9.3680ms  9.3680ms  [CUDA memcpy DtoH]\n",
            "                   40.77%  6.5130ms         2  3.2565ms  3.2527ms  3.2603ms  [CUDA memcpy HtoD]\n",
            "                    0.59%  94.558us         1  94.558us  94.558us  94.558us  add(float*, float*, float*)\n",
            "      API calls:   88.40%  154.30ms         3  51.433ms  88.906us  154.11ms  cudaMalloc\n",
            "                   10.01%  17.474ms         3  5.8247ms  3.4203ms  10.582ms  cudaMemcpy\n",
            "                    1.16%  2.0296ms         3  676.54us  167.66us  1.1382ms  cudaFree\n",
            "                    0.21%  362.95us         1  362.95us  362.95us  362.95us  cuDeviceTotalMem\n",
            "                    0.09%  165.22us        97  1.7030us     133ns  54.388us  cuDeviceGetAttribute\n",
            "                    0.09%  153.80us         1  153.80us  153.80us  153.80us  cudaDeviceSynchronize\n",
            "                    0.02%  32.859us         1  32.859us  32.859us  32.859us  cudaLaunchKernel\n",
            "                    0.01%  16.210us         1  16.210us  16.210us  16.210us  cuDeviceGetName\n",
            "                    0.00%  2.9630us         1  2.9630us  2.9630us  2.9630us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1790us         3     726ns     201ns  1.4800us  cuDeviceGetCount\n",
            "                    0.00%  1.4460us         2     723ns     316ns  1.1300us  cuDeviceGet\n",
            "                    0.00%     267ns         1     267ns     267ns     267ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWb6N439J73j"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8w-6yuRKok2"
      },
      "source": [
        "Si ejecutamos el programa utilizando un total de 256 threads por bloque y un solo bloque, es decir:\n",
        "```\n",
        "add<<<1, 256>>>(d_x, d_y, N);\n",
        "```\n",
        "Obtendríamos que solo se ha realizado la operación sobre 256 valores de los N posibles, esto mismo pasa utilizando 256 bloques y un thread en cada uno. \n",
        "\n",
        "Por otro lado, si ejecutamos dicha función sin quitar el bucle for que teníamos inicialmente en la función add:\n",
        "\n",
        "```\n",
        "__global__ void add(int n, float *x, float *y) {\n",
        "    for (int i =0; i < n; i++ ){\n",
        "        y[i]=x[i]+y[i];\n",
        "    }\n",
        "}\n",
        "```\n",
        "obtendríamos que cada thread lanzado realizaría todas las sumas, pudiendo ocurrir que se pisaran unas a otras\n",
        "\n",
        "```\n",
        "Suma de 1048576 elementos\n",
        "Número de errores: 3473\n",
        "Max error: 2\n",
        "```\n",
        "\n",
        "**Pendiente: Comentar <1,256> sin datarace y suma en la GPU con paralelismo de bloques.Prueba con mas Thread por bloque. Pruebas variando N y la complejidad de la\n",
        "operación matemática.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKdkqK87NQDw"
      },
      "source": [
        "## Suma o resta de matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etw0Z2fcGlaD",
        "outputId": "c1be5066-75a8-4bf1-c474-525eca8218b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile suma_mat.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// __global__ indica que se ejecuta en el dispositivo, por lo tanto\n",
        "// x, y,deben apuntar a memoria en el dispositivo.\n",
        "\n",
        "__global__ void add(float *x, float *y, int row, int col) {\n",
        "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int j = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int index = i*row + j;\n",
        "    y[index] = x[index] + y[index];\n",
        "}\n",
        "\n",
        "// Definimos las dimensiones de las matrices\n",
        "#define R 512\n",
        "#define C 512\n",
        "// Define number of threads per block\n",
        "#define THREADS_PER_BLOCK 32\n",
        "\n",
        "int main(void) {\n",
        "    // Inicializamos las variables\n",
        "    float *x, *y;                 // = new float[N];\n",
        "    float *d_x, *d_y;             // = new float[N]; \n",
        "    // Calculamos el tamaño de las matrices\n",
        "    int size = C*R*sizeof(float);\n",
        "\n",
        "    // Reservamos memoria en el dispositivo.\n",
        "    cudaMalloc((void **)&d_x, size);\n",
        "    cudaMalloc((void **)&d_y, size);\n",
        "\n",
        "    // Reservamos memoria en el host, trataremos las matrices como vectores\n",
        "    // con la dimension correspondiente y accederemos a ellos usando 2 indices.\n",
        "    x = (float *)malloc(R*C*sizeof(float*));\n",
        "    y = (float *)malloc(R*C*sizeof(float*));\n",
        "\n",
        "    // Inicializamos los datos\n",
        "    for (int i =0; i < R; i++ ){\n",
        "        for (int j = 0; j < C; j++){\n",
        "            x[i*R + j] = 1.0f;\n",
        "            y[i*R + j] = 2.0f;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Copiamos los valores al dispositivo\n",
        "    cudaMemcpy(d_x, x, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_y, y, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Creamos los objetos dim3 tanto para el Grid como para el bloque.\n",
        "    // Utilizamos un Grid de (R/THREADS_PER_BLOCK, C/THREADS_PER_BLOCK) bloques\n",
        "    // de forma que realicemos todas las operaciones (R, C).\n",
        "    dim3 dimGrid ( R/THREADS_PER_BLOCK , C/THREADS_PER_BLOCK ,1 ) ;\n",
        "    // Definimos la dimension del bloque.\n",
        "    dim3 dimBlock( THREADS_PER_BLOCK, THREADS_PER_BLOCK, 1 ) ;\n",
        "    // En ambos casos utilizamos un 1 para la tercera dimensión pues vamos a \n",
        "    // sumar matrices 2D.\n",
        "\n",
        "    // Lamzamos la función en el dispositivo.\n",
        "    add<<<dimGrid, dimBlock>>>(d_x, d_y, R, C);\n",
        "\n",
        "    // Esperamos que se realicen todas las operaciones.\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copiamos el resultado al host.\n",
        "    cudaMemcpy(y, d_y, size, cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    // Comprobamos los resultados\n",
        "    float maxError = 0.0f;\n",
        "    int contError = 0;\n",
        "   \n",
        "    for (int i=0; i <R; i++){\n",
        "        for (int j = 0; j < C; j++){\n",
        "          maxError=fmax(maxError,fabs(y[i*R + j]-3.0f));\n",
        "          if (y[i*R + j] != 3.0) contError++; \n",
        "        }\n",
        "    }\n",
        "    std::cout << \"suma de \" << R*C << \" Elementos\" << std::endl;\n",
        "    std::cout << \"Número de Errores: \" << contError << std::endl;\n",
        "    std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "    // Liberamos la memoria tanto en el host como en el dispositivo.\n",
        "    free(x); free(y);\n",
        "    cudaFree(d_x); cudaFree(d_y);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting suma_mat.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvCVjyGaI4MU"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma_mat.cu -o suma_mat -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrxmZhL2L97F",
        "outputId": "cf93907d-938b-44bb-f4c6-b4eb9c361cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!./suma_mat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "suma de 262144 Elementos\n",
            "Número de Errores: 0\n",
            "Max error: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_X9ir7X3C2N"
      },
      "source": [
        "## Spencil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fui3Dwac8ras"
      },
      "source": [
        "Primero compile y ejecute el código sin usar memoria compartida.\n",
        "Hacer un perfil con nvprof y sacar el comportamiento temporal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3QOalW-8vTe",
        "outputId": "e78a3843-db80-43b6-eee9-6f5a367c311a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 1destencilexercise_v1.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    // Indice global de la posicion central del spencil\n",
        "    int index = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += in[index + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[index-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 1destencilexercise_v1.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZIbKVxZ83NT",
        "outputId": "7541cc59-6ec7-459b-a4a2-a87ba6bc0b2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 1destencilexercise_v1.cu -o 1destencilexercise -lcudadevrt\n",
        "!./1destencilexercise"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUCCESS!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCgjJYPn9EL6"
      },
      "source": [
        "Comprobamos el comportamiento temporal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBXiKoal9DHy",
        "outputId": "7670b839-fbe5-4498-9e3a-83cc8ba7ef4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!nvprof ./1destencilexercise"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==575== NVPROF is profiling process 575, command: ./1destencilexercise\n",
            "SUCCESS!\n",
            "==575== Profiling application: ./1destencilexercise\n",
            "==575== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   37.81%  4.4160us         1  4.4160us  4.4160us  4.4160us  [CUDA memcpy DtoH]\n",
            "                   36.16%  4.2240us         1  4.2240us  4.2240us  4.2240us  [CUDA memcpy HtoD]\n",
            "                   26.03%  3.0400us         1  3.0400us  3.0400us  3.0400us  stencil_1d(int*, int*)\n",
            "      API calls:   99.61%  205.42ms         2  102.71ms  7.3560us  205.41ms  cudaMalloc\n",
            "                    0.18%  367.52us         1  367.52us  367.52us  367.52us  cuDeviceTotalMem\n",
            "                    0.07%  152.23us        97  1.5690us     145ns  64.354us  cuDeviceGetAttribute\n",
            "                    0.06%  127.92us         2  63.961us  14.654us  113.27us  cudaFree\n",
            "                    0.04%  84.581us         2  42.290us  33.218us  51.363us  cudaMemcpy\n",
            "                    0.02%  31.233us         1  31.233us  31.233us  31.233us  cuDeviceGetName\n",
            "                    0.01%  25.791us         1  25.791us  25.791us  25.791us  cudaLaunchKernel\n",
            "                    0.00%  3.5840us         1  3.5840us  3.5840us  3.5840us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8140us         3     604ns     171ns  1.1390us  cuDeviceGetCount\n",
            "                    0.00%  1.2090us         2     604ns     310ns     899ns  cuDeviceGet\n",
            "                    0.00%     290ns         1     290ns     290ns     290ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxnzXBR5_FIv"
      },
      "source": [
        "Al no utilizar memoria compartida, la mayoria del tiempo se dedica a copia de elementos.\n",
        "\n",
        "**TODO**: 2. Use el generador de perfiles para determinar cuál es el problema.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV_FLBxDRxaN",
        "outputId": "226a5f8c-e985-4fea-cdcc-103b2a15b82e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 1destencilexercise_v2.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2*RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "    //__syncthreads ();\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "  for (int j = 0; j < 100; j++){\n",
        "      unsigned int i;\n",
        "      int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "      int *d_in, *d_out;\n",
        "\n",
        "    // Initialize host data\n",
        "    for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "      h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "    // Allocate space on the device\n",
        "    cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "    cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "    // Copy input data to device\n",
        "    cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "    stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "    cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "    // Verify every out value is 7\n",
        "    for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "      if (h_out[i] != 7)\n",
        "      {\n",
        "        printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "        break;\n",
        "      }\n",
        "\n",
        "    // Free out memory\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "    cudaDeviceReset();\n",
        "\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 1destencilexercise_v2.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBv_sOG636b"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 1destencilexercise_v2.cu -o 1destencilexercise -lcudadevrt"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez-d6A9M6-pb",
        "outputId": "0d63aff3-5b2b-4543-9f45-b8d48fccef7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!./1destencilexercise"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[128] == 4 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[128] == 4 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[128] == 4 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[128] == 4 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[128] == 4 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[128] == 4 != 7\n",
            "Element h_out[95] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[63] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n",
            "Element h_out[61] == 6 != 7\n",
            "Element h_out[62] == 6 != 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_ILjJYw7wu9",
        "outputId": "e3c0632a-c1b2-40d4-b26b-5650b1db0fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 1destencilexercise_v3.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2*RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "    __syncthreads ();\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "      unsigned int i;\n",
        "      int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "      int *d_in, *d_out;\n",
        "\n",
        "    // Initialize host data\n",
        "    for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "      h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "    // Allocate space on the device\n",
        "    cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "    cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "    // Copy input data to device\n",
        "    cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "    stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "    cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "    // Verify every out value is 7\n",
        "    for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "      if (h_out[i] != 7)\n",
        "      {\n",
        "        printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "        break;\n",
        "      }\n",
        " \n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "\n",
        "    // Free out memory\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 1destencilexercise_v3.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgOtQ22UDzQ1",
        "outputId": "0e0a3809-0211-46b4-9f7e-bdda6ceaacd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 1destencilexercise_v3.cu -o 1destencilexercise -lcudadevrt\n",
        "!./1destencilexercise"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUCCESS!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Re8KVmD263",
        "outputId": "c2108224-a17c-46d7-d074-56c59ee13fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!nvprof ./1destencilexercise"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==1553== NVPROF is profiling process 1553, command: ./1destencilexercise\n",
            "SUCCESS!\n",
            "==1553== Profiling application: ./1destencilexercise\n",
            "==1553== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   37.37%  4.4480us         1  4.4480us  4.4480us  4.4480us  [CUDA memcpy DtoH]\n",
            "                   35.48%  4.2240us         1  4.2240us  4.2240us  4.2240us  [CUDA memcpy HtoD]\n",
            "                   27.15%  3.2320us         1  3.2320us  3.2320us  3.2320us  stencil_1d(int*, int*)\n",
            "      API calls:   99.49%  192.92ms         2  96.459ms  6.2670us  192.91ms  cudaMalloc\n",
            "                    0.27%  525.54us         1  525.54us  525.54us  525.54us  cuDeviceTotalMem\n",
            "                    0.08%  157.21us        97  1.6200us     146ns  64.270us  cuDeviceGetAttribute\n",
            "                    0.07%  127.07us         2  63.535us  13.789us  113.28us  cudaFree\n",
            "                    0.04%  76.101us         2  38.050us  30.751us  45.350us  cudaMemcpy\n",
            "                    0.03%  67.482us         1  67.482us  67.482us  67.482us  cudaLaunchKernel\n",
            "                    0.02%  31.431us         1  31.431us  31.431us  31.431us  cuDeviceGetName\n",
            "                    0.00%  3.4280us         1  3.4280us  3.4280us  3.4280us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1240us         3     708ns     202ns  1.5350us  cuDeviceGetCount\n",
            "                    0.00%  1.6460us         2     823ns     302ns  1.3440us  cuDeviceGet\n",
            "                    0.00%     304ns         1     304ns     304ns     304ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rArj_p1EPW1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
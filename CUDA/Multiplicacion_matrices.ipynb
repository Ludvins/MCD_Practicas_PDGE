{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiplicacion_matrices.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ludvins/Practicas_PDGE/blob/master/CUDA/Multiplicacion_matrices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Z3MGYPy7V5"
      },
      "source": [
        "*Luis Antonio Ortega Andrés    \n",
        "Antonio Coín Castro*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA2gC7QblMmO"
      },
      "source": [
        "# Ejercicio opcional: multiplicación de matrices\n",
        "\n",
        "Consideramos el problema de multiplicar dos matrices cuadradas $A$ y $B$ de dimensiones $N\\times N$, obteniendo como resultado una matriz $C$, también $N\\times N$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCv4c63Se-w7"
      },
      "source": [
        "## Multiplicación en CPU\n",
        "\n",
        "Realizamos la multiplicación en CPU, cuya eficiencia es $O(N^3)$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kti9K705k_NW",
        "outputId": "ebcdf39a-96dc-4f1e-a94e-679ebc0cf83a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile matmul_cpu.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 64\n",
        "\n",
        "void matrixMultCPU(float a[N][N], float b[N][N], float c[N][N]) {\n",
        "  int i, j, k;\n",
        "  for (i=0; i < N; i++) {\n",
        "    for (j = 0; j < N; j++) {\n",
        "      float sum = 0;\n",
        "      for (k = 0; k < N; k++) {\n",
        "        sum += a[i][k] * b[k][j];\n",
        "      }\n",
        "      c[i][j] = sum;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  float a[N][N], b[N][N], c[N][N];\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  /* inicializando variables con datos*/\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < N; j++) {\n",
        "      a[i][j] = 1.0;\n",
        "      b[i][j] = 2.0;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  int nIter=1000;\n",
        "  cudaEventRecord(start);\n",
        "  for (int i = 0; i < nIter; i++)\n",
        "    matrixMultCPU(a, b, c);\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "\n",
        "  float msecTotal = 0.0;\n",
        "  cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "  // Comprueba resultados\n",
        "  int errores = 0;\n",
        "  for (int y = 0; y < N; y++) {\n",
        "    for (int x = 0; x < N; x++) {\n",
        "        if (c[y][x] != 2*N) {\n",
        "          errores++;\n",
        "        }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"Producto de matrices %d x %d\\n\", N, N);\n",
        "  printf(\"Resultado \");\n",
        "  if (errores == 0){\n",
        "    printf(\"correcto\\n\");\n",
        "  }\n",
        "  else {\n",
        "    printf(\"incorrecto. Errores: %d\\n\", errores);\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  float msecPerKernelExecution = msecTotal / nIter;\n",
        "  double flopsPerMMull = 2.0 * N * N * N;\n",
        "  double gigaFlops = (flopsPerMMull * 1.0e-9) /\n",
        "    (msecPerKernelExecution / 1000.0);\n",
        "\n",
        "  printf(\"Tiempo medio de ejecución: %f ms\\n\", msecPerKernelExecution);\n",
        "  printf(\"GFLOPS: %f\\n\", gigaFlops);\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting matmul_cpu.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8I5n8pNl9lR",
        "outputId": "298add05-cd89-4388-962e-e81e92c5cd77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matmul_cpu.cu -o matmul_cpu -lcudadevrt\n",
        "!./matmul_cpu"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Producto de matrices 64 x 64\n",
            "Resultado correcto\n",
            "Tiempo medio de ejecución: 0.835759 ms\n",
            "GFLOPS: 0.627320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1cZYnHWnERJ"
      },
      "source": [
        "## Multiplicación en GPU\n",
        "\n",
        "Pasamos ahora a realizar la misma implementación pero en GPU. La adaptamos para un tamaño $N$ arbitrario, empleando siempre bloques de $8\\times 8$ threads (para aprovechar al máximo el paralelismo, como ya comentamos en el ejercicio sobre la suma de matrices), y un número de bloques igual a $(N+7)/8$. Diferenciamos según si se utiliza memoria compartida o no."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgkDxFIjnGbG"
      },
      "source": [
        "### Sin usar memoria compartida\n",
        "\n",
        "Implementamos en primer lugar el algoritmo más sencillo. Establecemos una estructura bidimensional de bloques y threads, y el thread con identificador $(i, j)$ (en coordenadas de grid) procesa la fila $j$-ésima y la columna $i$-ésima para obtener el elemento $C_{ij}$. De esta forma obtenemos del orden de $N^2$ threads, donde cada uno realiza $O(N)$ operaciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz3_Nv1KnOPk",
        "outputId": "a166325d-a4b5-47e9-820b-c310dd5740d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile matmul_gpu_v1.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 64\n",
        "#define TPB 8\n",
        "\n",
        "// Computa la multiplicación de matrices en GPU sin memoria compartida\n",
        "__global__ void matrixMultGPU(int n, float *a, float *b, float *c) {\n",
        "  int k;\n",
        "  float sum = 0;\n",
        "  int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  int row = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "  \n",
        "  if (row < n && col < n) {\n",
        "    for (k = 0; k < n; k++) {\n",
        "      sum += a[row * n + k] * b[k * n + col];\n",
        "    }\n",
        "    c[row * n + col] = sum;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  float a[N][N], b[N][N], c[N][N];\n",
        "  float *dev_a, *dev_b, *dev_c;\n",
        "  int size = N * N * sizeof(float);\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  cudaMalloc((void **) &dev_a, size);\n",
        "  cudaMalloc((void **) &dev_b, size);\n",
        "  cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "  // inicializando variables\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < N; j++) {\n",
        "      a[i][j] = 1.0;\n",
        "      b[i][j] = 2.0;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Tamaño de grid y bloque\n",
        "  dim3 dimGrid((N+TPB-1)/TPB, (N+TPB-1)/TPB);\n",
        "  dim3 dimBlock(TPB, TPB);\n",
        "\n",
        "  int nIter=1000;\n",
        "  cudaEventRecord(start);\n",
        "  for (int i=0; i <nIter; i++) {\n",
        "    matrixMultGPU<<<dimGrid, dimBlock>>>(N, dev_a, dev_b, dev_c);\n",
        "  }\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "\n",
        "  cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float msecTotal = 0.0;\n",
        "  cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "  // Comprueba resultados\n",
        "  int errores = 0;\n",
        "  for (int y = 0; y < N; y++) {\n",
        "    for (int x = 0; x < N; x++) {\n",
        "        if (c[y][x] != 2*N) {\n",
        "          errores++;\n",
        "          //printf(\"[%d][%d]=%f en vez de %d; \", y, x, c[y][x], 2*N);\n",
        "        }\n",
        "    }\n",
        "    //printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  cudaFree(dev_a);\n",
        "  cudaFree(dev_b);\n",
        "  cudaFree(dev_c);\n",
        "\n",
        "  printf(\"Producto de matrices %d x %d\\n\", N, N);\n",
        "  printf(\"Resultado \");\n",
        "  if (errores == 0){\n",
        "    printf(\"correcto\\n\");\n",
        "  }\n",
        "  else {\n",
        "    printf(\"incorrecto. Errores: %d\\n\", errores);\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  float msecPerKernelExecution = msecTotal / nIter;\n",
        "  double flopsPerMMull = 2.0 * N * N * N;\n",
        "  double gigaFlops = (flopsPerMMull * 1.0e-9) /\n",
        "    (msecPerKernelExecution / 1000.0);\n",
        "\n",
        "  printf(\"GFLOPS: %f\\n\", gigaFlops);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting matmul_gpu_v1.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWnT1dWEnYVk",
        "outputId": "c3360324-253c-419b-d436-0d40b6dd0deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matmul_gpu_v1.cu -o matmul_gpu_v1 -lcudadevrt\n",
        "!nvprof ./matmul_gpu_v1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==3491== NVPROF is profiling process 3491, command: ./matmul_gpu_v1\n",
            "Producto de matrices 64 x 64\n",
            "Resultado correcto\n",
            "GFLOPS: 56.663803\n",
            "==3491== Profiling application: ./matmul_gpu_v1\n",
            "==3491== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.89%  7.9497ms      1000  7.9490us  7.8720us  12.641us  matrixMultGPU(int, float*, float*, float*)\n",
            "                    0.07%  5.6320us         2  2.8160us  2.6560us  2.9760us  [CUDA memcpy HtoD]\n",
            "                    0.04%  2.9120us         1  2.9120us  2.9120us  2.9120us  [CUDA memcpy DtoH]\n",
            "      API calls:   94.48%  168.93ms         2  84.466ms  1.2010us  168.93ms  cudaEventCreate\n",
            "                    3.63%  6.4997ms      1000  6.4990us  4.4100us  36.539us  cudaLaunchKernel\n",
            "                    1.36%  2.4318ms         1  2.4318ms  2.4318ms  2.4318ms  cudaEventSynchronize\n",
            "                    0.21%  373.90us         1  373.90us  373.90us  373.90us  cuDeviceTotalMem\n",
            "                    0.10%  174.99us         3  58.330us  4.3020us  163.92us  cudaMalloc\n",
            "                    0.08%  134.20us        97  1.3830us     143ns  56.494us  cuDeviceGetAttribute\n",
            "                    0.07%  130.85us         3  43.616us  4.8820us  112.73us  cudaFree\n",
            "                    0.05%  80.839us         3  26.946us  13.642us  35.585us  cudaMemcpy\n",
            "                    0.02%  33.610us         1  33.610us  33.610us  33.610us  cuDeviceGetName\n",
            "                    0.00%  8.7890us         2  4.3940us  3.7490us  5.0400us  cudaEventRecord\n",
            "                    0.00%  3.0100us         1  3.0100us  3.0100us  3.0100us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1860us         1  2.1860us  2.1860us  2.1860us  cudaEventElapsedTime\n",
            "                    0.00%  1.8100us         3     603ns     170ns  1.1700us  cuDeviceGetCount\n",
            "                    0.00%  1.2470us         2     623ns     301ns     946ns  cuDeviceGet\n",
            "                    0.00%     283ns         1     283ns     283ns     283ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEYRfZ0HASP5"
      },
      "source": [
        "Vemos que se realiza adecuadamente el producto y no se producen errores. Además, el tiempo medio por ejecución es de tan solo 8 us, muy por debajo del tiempo medio de ejecución en CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8mM75xXndbv"
      },
      "source": [
        "## Memoria compartida con optimizaciones adicionales\n",
        "\n",
        "Pasamos ahora a optimizar la forma de cálculo del producto. En concreto hacemos tres optimizaciones:\n",
        "\n",
        "- Utilizamos memoria compartida para el acceso a los datos. De esta forma, definiremos \"mosaicos\" para hacer el producto de matrices por bloques, de manera independiente y optimizando el acceso a memoria. Así conseguimos que las matrices $A$ y $B$ se carguen solo $N/8$ veces en memoria, al contrario que antes que se cargaban $N$ veces. Necesitamos usar `__syncthreads()` para asegurarnos de que todos los datos necesarios han sido cargados.\n",
        "- Accedemos a los mosaicos de forma traspuesta para aumentar la eficiencia en memoria, debido a que se guardan en \"column-major\".\n",
        "- Desenrrollamos bucles para optimización del compilador. Utilizamos la directiva del precompilador `#pragma unroll`.\n",
        "\n",
        "También añadimos una mejora, y es que, además de permitir tamaños arbitrariamente grandes de $N$ (limitados solo por el tamaño máximo de grid), **permitimos que no tenga que ser potencia de 2**. En este caso debemos asegurarnos que los mosaicos que solo estén parcialmente rellenos tengan a 0 el resto de sus posiciones, para no influir en el cálculo. Como siempre, debemos comprobar que los índices en el acceso a matrices sean correctos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_GDTqyljLIW",
        "outputId": "9314acd6-5136-44bf-c3d5-d509978c37af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile matmul_gpu_v2.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 64\n",
        "#define TPB 8\n",
        "\n",
        "__global__ void matrixMultGPU2(int n, float* A, float* B, float* C) {\n",
        "    float sum = 0;\n",
        "    int tile;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int i = blockIdx.x * blockDim.x + tx;\n",
        "    int j = blockIdx.y * blockDim.y + ty;\n",
        "\n",
        "      // Mosaicos en memoria de bloque\n",
        "      __shared__ float As[TPB][TPB];\n",
        "      __shared__ float Bs[TPB][TPB];\n",
        "\n",
        "      // Recorre los mosaicos de A y B necesarios para computar la submatriz de C\n",
        "      for (tile = 0; tile < (n+TPB-1)/TPB; tile++){\n",
        "          // Carga los mosaicos de A y B en paralelo (y de forma traspuesta)\n",
        "          if ((ty + (tile*TPB))<n && i < n)\n",
        "            As[ty][tx] = A[(i * n) + (ty + (tile*TPB))];\n",
        "          else\n",
        "            As[ty][tx] = 0.0;\n",
        "          if ((tx + (tile*TPB))<n && j < n)\n",
        "            Bs[ty][tx] = B[((tx + (tile * TPB))*n) + j];\n",
        "          else\n",
        "            Bs[ty][tx] = 0.0;\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "            // Computa los resultados para la submatriz de C (también traspuestos)\n",
        "    #pragma unroll\n",
        "            for (int k = 0; k < TPB; k++)\n",
        "              sum += As[k][tx] * Bs[ty][k];\n",
        "\n",
        "            __syncthreads();\n",
        "      }\n",
        "      // Escribe en paralelo los resultados obtenidos por el bloque\n",
        "      if (i < n && j < n)\n",
        "        C[i * n + j] = sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  float a[N][N], b[N][N], c[N][N];\n",
        "  float *dev_a, *dev_b, *dev_c;\n",
        "  int size = N * N * sizeof(float);\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  cudaMalloc((void **) &dev_a, size);\n",
        "  cudaMalloc((void **) &dev_b, size);\n",
        "  cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "  // inicializando variables\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < N; j++) {\n",
        "      a[i][j] = 1.0;\n",
        "      b[i][j] = 2.0;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Tamaño de grid y bloque\n",
        "  dim3 dimGrid((N+TPB-1)/TPB, (N+TPB-1)/TPB);\n",
        "  dim3 dimBlock(TPB, TPB);\n",
        "\n",
        "  int nIter=1000;\n",
        "  cudaEventRecord(start);\n",
        "  for (int i=0; i <nIter; i++) {\n",
        "    matrixMultGPU2<<<dimGrid, dimBlock>>>(N, dev_a, dev_b, dev_c);\n",
        "  }\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "\n",
        "  cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float msecTotal = 0.0;\n",
        "  cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "  // Comprueba resultados\n",
        "  int errores = 0;\n",
        "  for (int y = 0; y < N; y++) {\n",
        "    for (int x = 0; x < N; x++) {\n",
        "        if (c[y][x] != 2*N) {\n",
        "          errores++;\n",
        "          //printf(\"[%d][%d]=%f en vez de %d; \", y, x, c[y][x], 2*N);\n",
        "        }\n",
        "    }\n",
        "    //printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  cudaFree(dev_a);\n",
        "  cudaFree(dev_b);\n",
        "  cudaFree(dev_c);\n",
        "\n",
        "  printf(\"Producto de matrices %d x %d\\n\", N, N);\n",
        "  printf(\"Resultado \");\n",
        "  if (errores == 0){\n",
        "    printf(\"correcto\\n\");\n",
        "  }\n",
        "  else {\n",
        "    printf(\"incorrecto. Errores: %d\\n\", errores);\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  float msecPerKernelExecution = msecTotal / nIter;\n",
        "  double flopsPerMMull = 2.0 * N * N * N;\n",
        "  double gigaFlops = (flopsPerMMull * 1.0e-9) /\n",
        "    (msecPerKernelExecution / 1000.0);\n",
        "\n",
        "  printf(\"GFLOPS: %f\\n\", gigaFlops);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting matmul_gpu_v2.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4C940mGqTTN",
        "outputId": "3afa4d4f-ecfa-4266-aca3-5715f4014b68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matmul_gpu_v2.cu -o matmul_gpu_v2 -lcudadevrt\n",
        "!./matmul_gpu_v2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Producto de matrices 64 x 64\n",
            "Resultado correcto\n",
            "GFLOPS: 55.592693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e86K_MREECk2"
      },
      "source": [
        "Podemos probar a incrementar el tamaño de las matrices, modificarlo para que no sean potencia de 2, e incluso variar el número de hilos por bloque. Comprobamos que en todo caso el resultado obtenido con esta versión sigue siendo correcto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osuCaNfdEMfN",
        "outputId": "35fb7bd3-c777-4810-ac81-ad053ce6ab34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!sed -i '/#define N/c\\#define N 200' matmul_gpu_v2.cu\n",
        "!sed -i '/#define TPB/c\\#define TPB 32' matmul_gpu_v2.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matmul_gpu_v2.cu -o matmul_gpu_v2 -lcudadevrt\n",
        "!./matmul_gpu_v2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Producto de matrices 200 x 200\n",
            "Resultado correcto\n",
            "GFLOPS: 105.132073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER-HzBhcngz0"
      },
      "source": [
        "## Medición de tiempos\n",
        "\n",
        "Medimos los tiempos medios de ejecución (en 1000 ejecuciones) y los GFLOPS con/sin optimizaciones, usando tamaños de bloque $TPB=8$ y $TPB=32$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To0y0UbYxsLC"
      },
      "source": [
        "### Sin memoria compartida"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmRgc1a1ySmc"
      },
      "source": [
        "!sed -i '/#define TPB/c\\#define TPB 8' matmul_gpu_v1.cu\n",
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matmul_gpu_v1.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matmul_gpu_v1.cu -o matmul_gpu_v1 -lcudadevrt && nvprof ./matmul_gpu_v1; done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0YOILXGYmdN"
      },
      "source": [
        "**Sin memoria compartida, bloques de $8\\times 8$, tiempos medios en $\\mu$sec**\n",
        "\n",
        "| Tamaño de matriz | CPU->GPU | GPU -> CPU | Ejecución | Ratio comparado con 128x128 | GFLOPs     |\n",
        "|:------------------:|:----------:|:------------:|:-----------:|:-----------------------------:|:------------:|\n",
        "| 16x16            | 1.66   | 1.88   | 2.77    |          0.01                   | 1.19   |\n",
        "| 32x32            | 1.87  | 2.01    | 3.69   |               0.09              | 11.09  |\n",
        "| 64x64            | 2.8  | 2.91    | 7.95   |              0.41               | 49.46     |\n",
        "| 128x128          | 6.91  | 6.84    | 34.01  |                 1            | 118.93 |\n",
        "| 512x512          | 89.63 | 84.51   | 856.27   |                 2.63            | 313.22 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AsV9Nvmyb9g"
      },
      "source": [
        "!sed -i '/#define TPB/c\\#define TPB 32' matmul_gpu_v1.cu\n",
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matmul_gpu_v1.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matmul_gpu_v1.cu -o matmul_gpu_v1 -lcudadevrt && nvprof ./matmul_gpu_v1; done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ts3TXECdb6B"
      },
      "source": [
        "**Sin memoria compartida, bloques de $32\\times 32$, tiempos medios en $\\mu$sec**\n",
        "\n",
        "| Tamaño de matriz | CPU->GPU | GPU -> CPU | Ejecución | Ratio comparado con 128x128 | GFLOPs     |\n",
        "|:------------------:|:----------:|:------------:|:-----------:|:-----------------------------:|:------------:|\n",
        "| 16x16            | 1.34  | 1.34   | 1.76    |          0.01                  | 1.28   |\n",
        "| 32x32            | 2.01 | 2.01    | 9.88   |               0.04              | 5.87  |\n",
        "| 64x64            | 2.78 | 2.91    | 17.07   |              0.22               | 28.54     |\n",
        "| 128x128          | 6.70  | 6.81    | 31.46  |                 1            | 128.26 |\n",
        "| 512x512          | 89.93 | 80.93   | 478.87   |                 4.36            | 559.59 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHYIU441iHAp"
      },
      "source": [
        "Vemos que en general con bloques de $8\\times 8$ las operaciones son más rápidas hasta tamaño $128\\times 128$, donde comienza a ser más veloz utilizar bloques de $32\\times 32$ hilos.\n",
        "\n",
        "Los tiempos de copia de datos CPU->GPU y viceversa son bastante similares, entre sí y para los dos tamaños de bloque analizados. El cuello de botella es el ancho de banda a la hora del acceso a memoria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swh-E3VKxm92"
      },
      "source": [
        "### Con memoria compartida y optimizaciones\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XEt4RM9KnlP"
      },
      "source": [
        "!sed -i '/#define TPB/c\\#define TPB 8' matmul_gpu_v2.cu\n",
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matmul_gpu_v2.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matmul_gpu_v2.cu -o matmul_gpu_v2 -lcudadevrt && nvprof ./matmul_gpu_v2; done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLUhvrLTe3zk"
      },
      "source": [
        "**Con memoria compartida, bloques de $8\\times 8$, tiempos medios en $\\mu$sec**\n",
        "\n",
        "| Tamaño de matriz | CPU->GPU | GPU -> CPU | Ejecución | Ratio comparado con 128x128 | GFLOPs     |\n",
        "|:------------------:|:----------:|:------------:|:-----------:|:-----------------------------:|:------------:|\n",
        "| 16x16            | 1.52  | 1.34   | 1.48    |          0.008                  | 1.02   |\n",
        "| 32x32            | 1.90 | 2.01    | 4.56   |               0.09              | 11.12  |\n",
        "| 64x64            | 2.80 | 2.88   | 8.3   |              0.45               | 54.98     |\n",
        "| 128x128          | 6.83  | 6.88    | 33.49  |                 1            | 120.78 |\n",
        "| 512x512          | 90.91 | 80.83   | 822.93  |                 2.7            | 325.90 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxrxfoa0v2X7"
      },
      "source": [
        "!sed -i '/#define TPB/c\\#define TPB 32' matmul_gpu_v2.cu\n",
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matmul_gpu_v2.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matmul_gpu_v2.cu -o matmul_gpu_v2 -lcudadevrt && nvprof ./matmul_gpu_v2; done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d3wjc4bf0_a"
      },
      "source": [
        "**Con memoria compartida, bloques de $32\\times 32$, tiempos medios en $\\mu$sec**\n",
        "\n",
        "| Tamaño de matriz | CPU->GPU | GPU -> CPU | Ejecución | Ratio comparado con 128x128 | GFLOPs     |\n",
        "|:------------------:|:----------:|:------------:|:-----------:|:-----------------------------:|:------------:|\n",
        "| 16x16            | 1.34  | 1.34   | 4.11    |          0.01                  | 1.30   |\n",
        "| 32x32            | 1.90 | 2.08    | 17.07   |               0.04             | 3.57  |\n",
        "| 64x64            | 2.83| 2.97   | 28.45   |              0.22               | 17.59     |\n",
        "| 128x128          | 6.86  | 6.94    | 51.28  |                 1            | 79.83 |\n",
        "| 512x512          | 92.37 | 97.34   | 795.86  |                 4.26            | 336.99 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "026_mkRViqQZ"
      },
      "source": [
        "En este caso observamos el mismo fenómeno que anteriormente: las operaciones con bloques de $8\\times 8$ son más rápidas en general, excepto en el caso de matrices $512\\times 512$, donde los bloques de $32\\times 32$ consiguen una pequeña mejora.\n",
        "\n",
        "Comparando estas versiones con las correspondientes que no utilizan memoria compartida, vemos que la aceleración se obtiene solo para tamaños grandes de matrices. Sin embargo, hay una excepción: en el caso de tamaño de bloque $32\\times 32$ obtenemos unos 330 GFLOPS usando memoria compartida, y más de 550 GFLOPS sin usarla.\n",
        "\n",
        "También podemos ver que el incremento en el ratio (frente a tamaño $128\\times 128$) es más sustancial en el caso de $512\\times 512$, lo que refuerza la idea de que se obtiene mayor aceleración para tamaños más grandes de matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct_pD_yDLM2B"
      },
      "source": [
        "### Otro tipo de dato\n",
        "\n",
        "Usamos matrices de tipo `double` en vez de `float`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC01yGu_wDUZ"
      },
      "source": [
        "!sed 's/float/double/g' matmul_gpu_v2.cu > matmul_gpu_v2_double.cu\n",
        "!sed -i '/double msecTotal = 0.0/c\\float msecTotal = 0.0;' matmul_gpu_v2_double.cu"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3bm4JjRwhMi"
      },
      "source": [
        "Medimos los tiempos en este caso, con memoria compartida y fijando $TPB=32$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqvp35AQw6kS"
      },
      "source": [
        "!sed -i '/#define TPB/c\\#define TPB 32' matmul_gpu_v2_double.cu\n",
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matmul_gpu_v2_double.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matmul_gpu_v2_double.cu -o matmul_gpu_v2_double -lcudadevrt && nvprof ./matmul_gpu_v2_double; done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nDJdg_2Xkx3"
      },
      "source": [
        "**Con memoria compartida, bloques de $32\\times 32$, tipo de dato `double`, tiempos medios en $\\mu$sec**\n",
        "\n",
        "| Tamaño de matriz | CPU->GPU | GPU -> CPU | Ejecución | Ratio comparado con 128x128 | GFLOPs     |\n",
        "|:------------------:|:----------:|:------------:|:-----------:|:-----------------------------:|:------------:|\n",
        "| 16x16            | 1.36  | 1.37   | 12.28    |          0.02                  | 0.63   |\n",
        "| 32x32            | 2.20 | 2.30    | 37.60   |               0.05             | 1.68  |\n",
        "| 64x64            | 4.17 | 4.44   | 69.33   |              0.24               | 7.42     |\n",
        "| 128x128          | 14.30  | 12.41    |132.86  |                 1            | 31.27 |\n",
        "| 512x512          | 329.74 | 164.04   | 1581.2  |                 5.44            | 169.69 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIhXXe9ulWDE"
      },
      "source": [
        "Si comparamos esta tabla con la correspondiente al caso de memoria compartida y bloques de $32\\times 32$ usando floats, vemos que los tiempos de ejecución son aproximadamente el doble, y por tanto los GLOPS descienden hasta algo más de la mitad. Esto tiene sentido, ya que el tamaño de un `double` es 8 bytes, mientras que el de un `float` es 4 bytes. Como el cuello de botella era precisamente el ancho de banda de memoria, este cambio ralentiza nuestros programas.\n",
        "\n",
        "Sin embargo, cabe destacar que en este caso es donde conseguimos el mayor ratio en GFLOPS de $512\\times 512$ comparado con $128\\times 128$, llegando a ser más de 5 veces mayor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPgZy3x_LdbF"
      },
      "source": [
        "### Comparación con el programa de ejemplo de CUDA\n",
        "\n",
        "Finalmente, comparamos los resultados con el programa de ejemplo *built-in* que proporciona CUDA para hacer el producto de matrices. Lo hacemos con los parámetros por defecto (usa bloques de $32\\times 32$) y con un tamaño de matrices de $512\\times 512$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2Qof2J8LcRh",
        "outputId": "73f4f61e-e0b8-4dc3-9175-b8c0a441d45a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /usr/local/cuda/samples/0_Simple/matrixMul\n",
        "!make"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/samples/0_Simple/matrixMul\n",
            "/usr/local/cuda-10.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o matrixMul matrixMul.o \n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp matrixMul ../../bin/x86_64/linux/release\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWudApOV3M15",
        "outputId": "f2eb77c2-e0d8-49e7-9c34-107439395882",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvprof ./matrixMul -wA=512 -wB=512 -hA=512 -hB=512"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Matrix Multiply Using CUDA] - Starting...\n",
            "==5088== NVPROF is profiling process 5088, command: ./matrixMul -wA=512 -wB=512 -hA=512 -hB=512\n",
            "GPU Device 0: \"Tesla T4\" with compute capability 7.5\n",
            "\n",
            "MatrixA(512,512), MatrixB(512,512)\n",
            "Computing result using CUDA Kernel...\n",
            "done\n",
            "Performance= 1012.51 GFlop/s, Time= 0.265 msec, Size= 268435456 Ops, WorkgroupSize= 1024 threads/block\n",
            "Checking computed result for correctness: Result = PASS\n",
            "\n",
            "NOTE: The CUDA Samples are not meant for performancemeasurements. Results may vary when GPU Boost is enabled.\n",
            "==5088== Profiling application: ./matrixMul -wA=512 -wB=512 -hA=512 -hB=512\n",
            "==5088== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.66%  79.597ms       301  264.44us  259.91us  270.44us  void MatrixMulCUDA<int=32>(float*, float*, float*, int, int)\n",
            "                    0.24%  191.78us         2  95.891us  88.387us  103.40us  [CUDA memcpy HtoD]\n",
            "                    0.10%  80.611us         1  80.611us  80.611us  80.611us  [CUDA memcpy DtoH]\n",
            "      API calls:   69.44%  186.68ms         3  62.226ms  5.1920us  186.59ms  cudaMalloc\n",
            "                   28.68%  77.091ms         1  77.091ms  77.091ms  77.091ms  cudaEventSynchronize\n",
            "                    0.88%  2.3555ms       301  7.8250us  4.9410us  59.920us  cudaLaunchKernel\n",
            "                    0.53%  1.4266ms         3  475.55us  247.25us  807.43us  cudaMemcpy\n",
            "                    0.12%  329.37us         1  329.37us  329.37us  329.37us  cuDeviceTotalMem\n",
            "                    0.10%  279.71us         1  279.71us  279.71us  279.71us  cudaDeviceSynchronize\n",
            "                    0.09%  250.36us         3  83.452us  33.898us  137.38us  cudaFree\n",
            "                    0.09%  233.95us         2  116.98us  106.81us  127.14us  cudaGetDeviceProperties\n",
            "                    0.05%  129.52us        97  1.3350us     150ns  54.621us  cuDeviceGetAttribute\n",
            "                    0.01%  25.283us         1  25.283us  25.283us  25.283us  cuDeviceGetName\n",
            "                    0.00%  9.7740us         2  4.8870us  4.5900us  5.1840us  cudaEventRecord\n",
            "                    0.00%  8.1330us         2  4.0660us     782ns  7.3510us  cudaEventCreate\n",
            "                    0.00%  5.2820us         1  5.2820us  5.2820us  5.2820us  cudaEventElapsedTime\n",
            "                    0.00%  3.3150us         1  3.3150us  3.3150us  3.3150us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.0220us         1  3.0220us  3.0220us  3.0220us  cudaSetDevice\n",
            "                    0.00%  2.1850us         3     728ns     159ns  1.5280us  cuDeviceGetCount\n",
            "                    0.00%  1.4190us         2     709ns     232ns  1.1870us  cuDeviceGet\n",
            "                    0.00%  1.1410us         1  1.1410us  1.1410us  1.1410us  cudaGetDeviceCount\n",
            "                    0.00%     279ns         1     279ns     279ns     279ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mez0G64IClhA"
      },
      "source": [
        "Vemos que en este caso se alcanzan los 1000 GFLOPS, superando por mucho a todos nuestros intentos anteriores."
      ]
    }
  ]
}